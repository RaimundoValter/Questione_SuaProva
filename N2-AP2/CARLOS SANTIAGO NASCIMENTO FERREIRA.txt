##<<1>>##
Provavelmente, o modelo está com Overfitting. O modelo está tentando aprender demais, pegando ruídos ou até mesmo informações irrelevante a cerca do problema. O gráfico do Overfitting é justamente esse, através das épocas (vezes que o meu passo de treino percorre o meu conjunto de dados de treino) há uma queda no loss (perca) conjunto de treino, mas aumento do loss para o meu conjunto de validação. Existem vários métodos elegantes para sair desse problema, um dos que eu vi foi usar técnicas de adaptação, como o algoritmo ADAM, que ele regula automaticamente o aprendizado da rede neural artificial a cada época, evitando overfitting ou underfitting. Um outro possível caminho para remediar a situação, é usar técnicas como Early-Stop (para de treinar o algoritmo quando o Loss chegar a um determinado valor pré-definido).
##<<2>>##
O Momentum é uma técnica usada para otimização, onde a cada época de treinamento, ele reajusta a taxa de aprendizagem e ajuda ela a analisar e convergir para perto do valor esperado. É similar ADAM, que regula automaticamente o aprendizado da rede neural artificial, evitando overfitting ou underfitting. Um outro possível caminho para remediar a situação, é usar técnicas de Early-Stop (para de treinar o algoritmo quando o Loss chegar a um determinado valor pré-definido) ou Dropout (que exclui aleatoriamente alguns neurônios para diminuir a chance de oscilação do loss).

##<<3>>## 
Arquitetura: Na nossa Input Layer, teríamos n neurônios para a entrada (temos 18 totais atributos de informações clínicas de entradas mas também temos os metadados da imagem, mas a questão não dá valores nem fala sobre esses metadados, então assumiremos que a quantidade de neurônios será de 18 + qtdMetadadosImg), eu pessoalmente colocaria 2 hidden layers de 25 neurônios com função de ativação ReLU (ou até mesmo, uma função customizada, onde diferente do ReLU que classifica somente entre 0 e superiores a 0 (incluso ele próprio)), para dividir melhor o processo de processamento dos dados de treino. A camada de saída teria três neurônios (pois são três possíveis diagnósticos: saudável, pneumonia viral ou pneumonia bacteriana). As estratégias para evitar overfitting, eu poderia utilizar uma técnica de otimização como o Early-Stop (que para de treinar o algoritmo quando o loss chegar a um determinado valor).
Pré-processamento de dados: Para preservar os metadados, como estamos trabalhando com imagens, a minha ideia era utilizar essa imagem como tensor, aplicando filtros para preservar as características dessas imagens, criando o meu feature map.
Treinamento e Avaliação: Para treinamento, eu separaria o meu conjunto em 80% treino, 20% validação e teste. Para o meu batch-size (ou seja, a quantidade de amostras que eu usaria a cada passo de treinamento, colocaria 50). Para cada batch, aplicaria a minha função de ativação, calcularia o meu loss e após calculado, regularia os pesos, bias e taxa de aprendizagm, propagando da minha camada de entrada até a minha camada de saída. A cada final de época de treino, testaria o modelo com o meu conjunto de validação. Se conseguisse um desempenho acima de 85%, o modelo estaria pronto para produção.
Exemplo de Aplicação: 
	- Um médico utiliza o modelo na clínica, passa as informações em CSV do paciente junto das fotos do raio x. O modelo retorna uma porcentagem para cada uma das classes: saudável, pneumonia viral ou bacteriana.
	
##<<4>>## 
O Dropout funciona da seguinte maneira: Ao treinar o algoritmo, ele pode ser predefinido para excluir uma porcentagem de neurônios. Por exemplo, eu posso escolher excluir 20% dos meus neurônios. Esse processo é feito de forma aleatória. Desta forma, é possível excluir informações de forma aleatória que poderiam ser ruidosas ou generalizadas até demais sem afetar a integridade do meu modelo, evitando por si só, um underfitting.
